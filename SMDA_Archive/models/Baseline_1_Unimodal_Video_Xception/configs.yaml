wandb:
  login: 
    key: "99975cd8f95af3ba68278d2484eb109af43a4a64" ### Login key / Don't change
  init: ### Ref: https://docs.wandb.ai/ref/python/init
    project: "SMDA_Project" ### Dont't change
    entity: "cmu_23f_iitp_smad_dfd" ### Your wandb profile name (=id)
    save_code: true ### Don't change
    group: "" ### Don't change / Ref: https://docs.wandb.ai/guides/runs/grouping
    job_type: "train" ### "data-preprocess", "train", "test", "visualize" etc...
    tags: ["Baseline_1", "Unimodal", "Video", "MesoInception"] ### [Network, Size, etc...]
    name: "MesoInception_128_3_1" ### "Network"_"Size"_"Version" | Version policy: v{Architecture change}_{Method/Block change}_{Layer/Minor change}
    notes: "Test [Baseline_1_Unimodal_Video_Xception] to our benchmark" ### Insert changes(plz write details)
    dir: "./wandb" ### Don't change
    resume: false ### Don't change #"auto"
    reinit: true ### Don't change # false
    magic: null ### Don't change
    config_exclude_keys: [] ### Don't change
    config_include_keys: [] ### Don't change
    anonymous: null ### Don't change
    mode: "online" ### Don't change
    allow_val_change: true ### Don't change
    force: false ### Don't change
    sync_tensorboard: false ### Don't change
    monitor_gym: false ### Don't change
    config:
      dataset:
        data_path: "/shared/s2/lab01/yunjinna/smda" # /home/lsy/laboratory/Research/idea4_MDFD/data
      dataloader:
        batch_size: 128 ## 64 ## acc 128 < 256
        pin_memory: true
        num_workers: 12
      model:
        num_classes: 2
      criterion: ### Ref: https://pytorch.org/docs/stable/nn.html#loss-functions
        name: "CrossEntropyLoss" ### Choose a torch.nn's class(=attribute) e.g. ["CrossEntropyLoss", "MSELoss", "Custom", ...] / You can build your criterion :)
      optimizer: ### Ref: https://pytorch.org/docs/stable/optim.html#algorithms
        name: "Adam" ### Choose a torch.optim's class(=attribute) e.g. ["Adam", "AdamW", "SGD", ...] / You can build your optimizer :)
        Adam: ### Add or modify instance & args using reference link
          lr: 0.0001 # 0.001 # 0.0001
          betas: [0.9, 0.999]
          eps: 1.0e-08
          #weight_decay: 1.0e-2
        AdamW:
          lr: 2.0e-3
          weight_decay: 1.0e-4
        SGD:
          lr: 2.0e-3
          momentum: 0.9
          weight_decay: 1.0e-4
        Custom:
          custom_arg1:
          custom_arg2:
      scheduler: ### Ref(+ find "How to adjust learning rate"): https://pytorch.org/docs/stable/optim.html#algorithms
        name: "StepLR" ### Choose a torch.optim.lr_scheduler's class(=attribute) e.g. ["StepLR", "ReduceLROnPlateau", "Custom"] / You can build your scheduler :)
        StepLR: ### Add or modify instance & args using reference link
          step_size: 5
          gamma: 0.5
        ReduceLROnPlateau:
          mode: "min"
          min_lr: 1.0e-10
          factor: 0.9
          patience: 5
        CosineAnnealingLR:
          T_max: 5
          eta_min: 1e-6
        Custom:
          custom_arg1:
          custom_arg2:
      engine:
        epoch: 3
        gpuid: "0" ### "0"(single-gpu) or "0, 1" (multi-gpu)